[["welcome-to-ebio4100-bioinformatics-in-the-mountains.html", "EBIO4100: Bioinformatics in the Mountains Welcome to EBIO4100: Bioinformatics in the Mountains! 0.1 How to use this ebook 0.2 Goals/Learning Objectives 0.3 How to succeed in this course: 0.4 Important Links", " EBIO4100: Bioinformatics in the Mountains Megan Frayer 2025-07-10 Welcome to EBIO4100: Bioinformatics in the Mountains! Taught by Megan Frayer July 2025 0.1 How to use this ebook This text will outline the activities that we will complete each afternoon during the course. This book should remain available on my GitHub page after the class Canvas site goes away. Some extra information you will see in this text: Keep Learning! Boxes like these will indicate resources where you can learn more! These resources are not necessary for our course, and will not be part of assignments– they are just for your own learning. Caution! Boxes like these will indicate common pitfalls or places where we will make assumptions for simplicity that you should NOT make in real data. 0.2 Goals/Learning Objectives 0.3 How to succeed in this course: Bioinformatic software is always being improved. This means it is not important to memorize commands, but to understand the types of data and challenges you will face. This course is meant to give you a starting point for digging deeper into the questions and work that might interest you in the future. 0.4 Important Links CURC On-Demand Portal CURC Documentation CURC Cheatsheet "],["computer-set-up-and-introduction-to-the-command-line.html", "Activity 1 Computer set up and Introduction to the command line 1.1 Get oriented to the cluster 1.2 First command 1.3 How to navigate a file system 1.4 Basic file manipulation 1.5 Assignment 1.6 If we have extra time…", " Activity 1 Computer set up and Introduction to the command line This activity corresponds to Day 1 powerpoints available on Canvas. The first thing we need to do is make sure everyone can log onto the cluster. We will use the cluster for the rest of the activities in this course. 1.1 Get oriented to the cluster The first thing we will need to do is make an account. Everyone will need to set up an account following these instructions. A member of the CU Research Computing staff will be on call to help us this afternoon. Watch this video to learn about the Alpine filesystem. If you would like more information on the cluster, please see the documentation on their website. You can also refer here if you need help during the course! Overall Documentation CURC Cheatsheet Now that we have accounts, let’s log in! Go to the OpenOnDemand site, and use the menu at the top to access the Alpine Shell. 1.2 First command When you have accessed the Alpine shell, type echo \"Hello World!\". The argument will be printed below your command. Congratulations, you’ve run your first command! 1.3 How to navigate a file system We will now familiarize ourselves with the file system. 1. List the contents of your project folder. Code ls /projects/usrname/ 2. Make a new folder in projects called day_1 and switch your location into that folder. Code mkdir /projects/usrname/day_1 cd /projects/usrname/day_1 3. What is the file path to this location? Code pwd 4. What are the command options for ls? Code ls --help 1.4 Basic file manipulation 5. Copy the file day1.txt from our shared course folder to your day_1 folder. All the files that you need to complete activities during this course will be found in this location. Code cp /scratch/alpine/mefr3284/ebio4100/day_1/day_1.txt ./ 6. Rename the file to day_1_original.txt. Code mv day_1.txt day_1_original.txt 7. Look at the whole file, then at just the first 5 lines, then at just the last 5 lines. Code less day_1_original.txt head -5 day_1_original.txt tail -n5 day_1_original.txt 8. Count the lines of the file that contain the word “apple.” Code wc -l day_1_original.txt grep apple day_1_original.txt &gt; day_1_apples.txt wc -l day_1_apples.txt grep [aA]pple day_1_original.txt | wc -l **9. Replace all instances of the word “woodchuck” with “whistlepig”. Code sed &#39;s/woodchuck/whistlepig/g&#39; day_1_original.txt &gt; day_1_mod.txt 1.5 Assignment Write a command that makes a new file from day_1_original.txt that contains the first three lines with the word apple in them. Give the output file the name yourlastname_day1.txt and copy it to the shared folder Assignments/day_1. 1.6 If we have extra time… When running jobs on the cluster, we will typically use scripts. Watch this video from CURC. Make a script in your day_1 folder called count_apples.sh. Fill out the script using the command you created for the assignment. Run with with sbatch count_apples.sh. This will give you a job ID. Use that to run sacct -j &lt;job ID&gt;. "],["fasta-files-and-visual-alignment-with-mafft.html", "Activity 2 FASTA files and visual alignment with MAFFT 2.1 Exploring a fasta file 2.2 Visual Alignment 2.3 MAFFT Multiple Sequence Alignment 2.4 Assignment", " Activity 2 FASTA files and visual alignment with MAFFT 2.1 Exploring a fasta file 1. Start by making a new folder in your scratch directory and copying today’s file there. Code cd /scratch/alpine/usrname mkdir day_2 cp /scratch/alpine/mefr3284/ebio4100/day_2/* day_2/ ls 2. Look at the file. Code less chickadee.fasta How are the headers marked? What information is included there? 3. How many samples are included in this file? Code grep &quot;&gt;&quot; chickadee.fasta | wc -l 4. We can search for patterns within the data using grep. Code grep --color=always &#39;TAAACAA&#39; chickadee.fasta | head -10 grep --color=always &#39;TA[TA]TA&#39; chickadee.fasta | head -10 5. How many times does the pattern TAAACAA appear in these sequences? Code grep -o &#39;TAAACAA&#39; chickadee.fasta | wc -l 6. How many potential start codons are there? Hint: The “start codon” initiates translation of an amino acid chain. The RNA codon is AUG. Code grep -o &#39;ATG&#39; chickadee.fasta | wc -l The command grep is really powerful! If you’re interested in trying more grep commands, here’s a useful list of things you can do with grep. 7. How long are each of these sequences? This sounds like an easy question, but it is harder than it looks! We need to take into account: Each read is split across multiple lines. Each individual is represented by both an identifier line and a sequence line. We want to count only the length of the sequence lines, but associate them with the sample lines. Using what we have learned so far, this would be challenging. But I think it is useful to think about. The structure of the fasta file (and the possible variations on it) make things challenging, and this problem gets bigger as the file formats get more complicated. That is why in general, it is a great idea to use existing programs to process data, particularly ones that are well documented and have been thoroughly tested. Code Because I do not want to leave the question unanswered, here is a command that will work. It uses awk, which is a really useful linux program for processing. awk &#39;/^&gt;/{if (l!=&quot;&quot;) print l; print; l=0; next}{l+=length($0)}END{print l}&#39; chickadee.fasta 2.2 Visual Alignment 8. In pairs, try to align these two sequences by hand. These are protein sequences, so each letter represents an amino acid. Code &gt;seq1 HEAGAWGHEE &gt;seq2 PAWHEAE How many ways can you align these? What if you allow gaps? 2.3 MAFFT Multiple Sequence Alignment Multiple sequence aligners do as the name suggests– align multiple sequences at once. There are a variety of algorithms implemented in a variety of programs that perform multiple alignment. Today, we will be using MAFFT (Multiple Alignment Fast Fourier Transform), which is one of the most popular programs and can implement multiple algorithms. Follow this link to the online implementation of MAFFT. We aren’t going to discuss the inner workings of multiple sequence alignment, but here is a video and two papers you can check out if you are interested. Lecture walking through the process An older but comprehensive comparison (Edgar and Batzoglou 2006) The most recent MAFFT paper (Katoh, Rozewicki, and Yamada 2019) 9. Copy and paste the sequences above into the text box and submit it. You should be able to use all of the default settings. Is this alignment the same one you came up with? 10. Now try running our chickadee dataset. You will need to download it from the cluster– I recommend using the OnDemand Portal. Look at the resulting alignment in the View tab. What has been added to the input data? Do you see any patterns in the data? (It’s okay if you don’t– this is a difficult way to look at data!) What other information can you get from MAFFT? 2.4 Assignment Download the new fasta file from MAFFT, and then upload it to the cluster using the OnDemand portal. Copy it to /scratch/alpine/mefr3284/Assignments/day_2/, adding your name to the file name. References Edgar, Robert C., and Serafim Batzoglou. 2006. “Multiple Sequence Alignment.” Current Opinion in Structural Biology 16 (3): 368–73. https://doi.org/https://doi.org/10.1016/j.sbi.2006.04.004. Katoh, Kazutaka, John Rozewicki, and Kazunori D Yamada. 2019. “MAFFT Online Service: Multiple Sequence Alignment, Interactive Sequence Choice and Visualization.” Briefings in Bioinformatics 20 (4): 1160–66. https://doi.org/10.1093/bib/bbx108. "],["alignment-of-whole-genome-data.html", "Activity 3 Alignment of whole genome data 3.1 Environmental variables 3.2 Explore the sequencing data and reference genome 3.3 FASTQC 3.4 Trimming 3.5 Alignment with BWA 3.6 Read Groups 3.7 How good was our alignment? 3.8 Assignment", " Activity 3 Alignment of whole genome data 3.1 Environmental variables Before we start today, we are going to learn about environmental variables, and create one that will allow us to skip typing out the path to the course folders. Variables are a useful way to store data that can be referenced in other commands. For example, we could store the name of the file super_long_file_name_37456392347283.txt in the variable $FILE. This would allow us to then refer to the file using the variable, instead of typing out the name: Code FILE=super_long_file_name_37456392347283.txt wc -l $FILE The variable $FILE will only be available in context in which it was created (such as during one terminal session, or one script). However, there is a way to create variables that will apply to all of your terminal sessions. You do this by modifying your .bashrc file. Your .bashrc file is in your home directory, but you might not be able to see it. Unix systems treat file names beginning with . as hidden files. To see them in the terminal, you can use ls -a ~/. You will see there are a lot of files here! Using either the terminal or the OnDemand file system, add the following lines to the end of your .bashrc file. Code export shared=/scratch/alpine/mefr3284/ebio4100 export turnin=/scratch/alpine/mefr3284/Assignments Now we can use the variables $shared and $turnin to refer to the folders where you get files for the activities and turn in your homework, respectively. This change will not take effect until you start a new terminal session (i.e. log out and log back in). You can set these variable names to whatever makes sense to you, but I will use these two names in the rest of the activities for consistency. 3.2 Explore the sequencing data and reference genome 1. Start by making a new folder in your scratch directory and copying today’s file there. Code cd /scratch/alpine/usrname mkdir day_3 cp $shared/day_3/* day_3/ cd day_3/ ls 2. Start an interactive job. Code sinteractive --partition=amilan -t 4:00:00 3. Look at the data we will be using today. What kinds of file formats are these? What kind of information do they contain? Code less mountain_chickadee_R1.fastq.gz 4. How many files are in the folder? How many individual read sequences does this correspond to? These files are gzipped (a specific type of file compression) to make them more compact (this is what the .gz on the end of the filename indicates). The command less works because it internally knows how to handle those types of files, but not all commands do. One way to deal with this is to use zcat. The command cat can print out the content of a file; zcat does the same, but in a way that can handle zipped files. Code # how many fastq files zcat mountain_chickadee_R1.fastq.gz | wc -l zcat mountain_chickadee_R2.fastq.gz | wc -l Fastq files have 4 lines for each read, so the number of lines divided by 4 should give you the number of reads. 5. What files are present in the reference genome folder? Code ls $shared/reference 6. What are the scaffolds of the chickadee reference genome? Code grep &#39;&gt;&#39; $shared/reference/moch.dovetail.reference.hap1.scaffolds.renamed.fasta 3.3 FASTQC The first step after getting new sequencing data (or any type of data) is to do quality control (QC). We can’t go through all of this information by hand, so we need to start analyzing this data in other ways. For fastq files, we can check them using FastQC. If we wanted to use FastQC on our own computers, we would need to download and install the program. The program is already available on the cluster, but it is stored in something called a module. 7. View the available modules and load FastQC. Code module avail module load fastqc On the Alpine cluster, modules are not available to be loaded on the login nodes. You should already be in an interactive job, but if you cannot view the list of available modules, you should check your location. 8. Run FastQC on the first file. Code fastqc mountain_chickadee_R1.fastq.gz 9. What output files does it make? Code ls Take a look at the html output file (you may need to download it from the OnDemand Portal). How do the results look? If you want more information on any of the stats, check out the documentation here. This site also has examples of good and bad data. 10. Repeat fastqc with the second set of reads. How do the results compare? Code fastqc mountain_chickadee_R2.fastq.gz 3.4 Trimming 11. Use trimmomatic to trim the reads. When you load the module, it will give you cluster-specific instructions for running the program. Code module load trimmomatic java -jar $TRIMMOMATIC PE mountain_chickadee_R1.fastq.gz mountain_chickadee_R2.fastq.gz -trimlog mountain.trimlog -baseout mountain_chickadee_trimmed ILLUMINACLIP:$ADAPTERS/TruSeq3-PE.fa:2:30:10 SLIDINGWINDOW:5:20 MINLEN:75 Let’s break down this command. java -jar $TRIMMOMATIC is our program. Here, $TRIMMOMATIC is an environmental variable set by the module when we loaded it. It tells the computer where to find the trimmomatic file. PE tells trimmomatic that we are going to give it paired end reads. mountain_chickadee_R1.fastq.gz mountain_chickadee_R2.fastq.gz are our input fastq files that we want it to trim. -trimlog mountain.trimlog names an file for trimmomatic to use as a log. This will give us information on each read that it processes. -baseout mountain_chickadee_trimmed will give this prefix to all the output fastqs. ILLUMINACLIP:$ADAPTERS/TruSeq3-PE.fa:2:30:10 tells trimmomatic how to deal with adapter sequences. First, $ADAPTERS/TruSeq3-PE.fa gives the location of the adapter file. These are stored on the cluster, and $ADAPTERS is another variable created when we loaded the module. The numbers provide thresholds for how closely the read must match the adapter sequence to be trimmed. SLIDINGWINDOW:5:20 defines quality filtering. Trimmomatic will go across the read in windows of 5 basepairs, and trim sites with a quality score below 20. MINLEN:75 tells trimmomatic to throw out any read that is less than 75 basepairs after trimming. There are other options we can use to tune our output, which you can read about here. After running this command, what output files have been made? 11. Rerun fastqc on one of the new files. Did it improve? Code fastqc mountain_chickadee_trimmed_1P A useful program called MultiQC can help visualize results for multiple steps of bioinformatic quality control. We won’t be using it in this course, but you can check it out here. 3.5 Alignment with BWA 12. Load bwa and samtools modules. Code module load bwa module load samtools 13. Align the reads with bwa mem, and then use samtools view to remove the low quality alignments. Code bwa mem -t 1 -M -R &#39;@RG\\tID:moch\\tLB:moch\\tPL:ILLUMINA\\tSM:moch&#39; $shared/reference/moch.dovetail.reference.hap1.scaffolds.renamed.fasta mountain_chickadee_trimmed_1P mountain_chickadee_trimmed_2P | samtools view -q20 -b &gt; mountain_chickadee.filtered.bam We will name this sample moch. Names for samples should be as simple as possible, while still being unique. They should not contain spaces. Also, the -b flag in the samtools command tells it that we want our output in the binary BAM format, which will be smaller than the more readable SAM file. What output files were made? 14. If we look at this file, we will notice that the locations of the reads are random. For many downstream analyses, we will want to sort it. We can also do this with SAMtools. The -T flag specifies a prefix for temporary files created during sorting. Code samtools view mountain_chickadee.filtered.bam | less samtools sort mountain_chickadee.filtered.bam -T tmp_mntchck &gt; mountain_chickadee.filtered.sorted.bam If we look at the file now, we can see that the reads are in order by location. Remember that we are working with BAM files here, rather than SAM files. BAM files are binary, we we will not be able to read them like text files or view them with less. In this case, we can use samtools to do the conversion for us, using commands like samtools view. 15. Here, we did filtering and sorting as separate steps, but they could’ve been done in a single step. You will notice we have also created several versions of this SAM file. Sometimes, you will want to keep these intermediate files for special analyses, but usually we do not need them. You could delete them once you’ve made your final file, but you could also skip creating these files altogether by using pipes to connect our commands. The pipe (|) tells the terminal to pass the output directly into the next command, rather than into a file. Code bwa mem -t 1 -M -R &#39;@RG\\tID:moch21\\tLB:moch21\\tPL:ILLUMINA\\tSM:moch21&#39; $shared/reference/moch.dovetail.reference.hap1.scaffolds.renamed.fasta mountain_chickadee_trimmed_1P mountain_chickadee_trimmed_2P | samtools view -q 20 -b | samtools sort -T tmp_mntchck &gt; mountain_chickadee.filtered.sorted.bam Note that the -b flag added to the samtools view command is what tells samtools we want a BAM file output, rather than SAM. 16. Finally, most programs will expect BAM files to be indexed. This allows programs to quickly navigate large files. Code samtools index mountain_chickadee.filtered.sorted.bam We can also see that a new file called mountain_chickadee.filtered.sorted.bam.bai was created. This is the index and downstream programs will expect it to be in the same folder as the main file. 3.6 Read Groups Earlier we used the -R flag to add a string to our file. This was the read group. Some downstream programs will require read group information. If we want to merge BAM files together, we will also want the reads assigned to groups. If you do not add a group, or if a downstream program requires different group information, you can add or change read groups in a bam file. There are several ways to do this, but below is an example using samtools. Code samtools addreplacerg -r &quot;ID:HNJTWDSX3:2&quot; -r &quot;LB:2&quot; -r &quot;PL:ILLUMINA&quot; -r &quot;PU:HNJTWDSX3:2:moch21&quot; -r &quot;SM:moch21&quot; mountain_chickadee.filtered.sorted.bam -o mountain_chickadee.filtered.sorted.newrg.bam samtools index mountain_chickadee.filtered.sorted.newrg.bam 3.7 How good was our alignment? 17. Use samtools flagstat to look at some metrics for our alignment. Code samtools flagstat mountain_chickadee.filtered.sorted.bam &gt; flagstat.txt cat flagstat.txt What information do we get from this file? As mentioned in lecture, SAM flags store a lot of information about a sequence. If you’d like to learn more about these encodings, check out the documentation (Page 7) or this YouTube explainer. You can look up the meaning of specific SAM flags here. 3.8 Assignment Create a script that will copy the files $shared/chickadee_data/MOCH/20H102_S21_L002_R1_001.fastq.gz and $shared/chickadee_data/MOCH/20H102_S21_L002_R2_001.fastq.gz to your scratch directory (you probably want to make a specific folder for this), and then trim, align, and sort the bam file. You can add the read group using the same read group information we used above. Try running the script, which will probably run past the end of class. Copy the script to the folder $turnin/day_3/ to turn it in. (Add your name to the file name!) Hint 1: Code The header of your script should look something like: #!/bin/bash #SBATCH --nodes=1 #SBATCH --ntasks=1 #SBATCH --time=6:00:00 #SBATCH --partition=amilan #SBATCH --output=bamfiles.out #SBATCH --qos=normal Hint 2: Code Do not forget to load the modules you need. Hint 3: Code To run a script, use: sbatch script_name.sh If you would like more information on the programs we tried today, here are links to check out. Trimmomatic Documentation BWA Documentation Samtools Documentation BCFtools Manual "],["variant-calling.html", "Activity 4 Variant calling 4.1 Remove duplicates from the BAM file 4.2 Call Variants with GATK 4.3 Call variants with bcftools 4.4 Are they all the same? 4.5 Assignment", " Activity 4 Variant calling Today we will use the bam file that we made in the last class to call variants (mountain_chickadee.filtered.sorted.bam). We will call variants using two popular approaches. If your file was deleted, you can copy a new one from the shared folder. 1. Make folders for today, and copy your file into it. Code cd /scratch/alpine/usrname mkdir day_4 cp $shared/day_4/* day_4/ cd day_4 4.1 Remove duplicates from the BAM file For many analyses, the last thing we need to do with the BAM file is remove duplicates. Reads sometimes get duplicated during library prep (the chemical process we use to prepare isolated DNA for sequencing). This will show up in our BAM file as reads that are completely identical from start to end. For most analyses, we want to remove these because they might give us false information about the frequency of variants they contain. To remove duplicates, we will use Picard. Picard is a large and powerful program, you can look at its manual for more information. After removing duplicates, we will need to re-index the file. 2. Remove duplicates and reindex the file. Code module load picard picard MarkDuplicates -INPUT mountain_chickadee.filtered.sorted.bam -OUTPUT mountain_chickadee.filtered.sorted.nodup.bam -METRICS_FILE mountain_chickadee.duplicate.metrics -REMOVE_DUPLICATES true -ASSUME_SORTED true -TMP_DIR tmp_mountain_chickadee -VALIDATION_STRINGENCY LENIENT samtools index mountain_chickadee.filtered.sorted.nodup.bam In the case of this small set of reads, there are no duplicates, so we have not changed anything in the file. If you look at the file mountain_chickadee.duplicate.metrics you can see the counts of reads that it examined. 4.2 Call Variants with GATK The first variant caller we will try is called GATK. This is one of the most common variant callers. It requires two steps. First, HaplotypeCaller is used to create a GVCF file. Second, GenotypeGVCFs will make out final VCF file. If we had multiple samples, we would run each separately in HaplotypeCaller, and then combine them before running GenotypeGVCFs. 3. Load the module and run HaplotypeCaller. Note: GATK provides cluster-specific instructions when you load the module. Code module load gatk gatk --java-options &quot;-Djava.io.tmpdir=$SLURM_SCRATCH&quot; HaplotypeCaller -R /scratch/alpine/mefr3284/ebio4100/reference/moch.dovetail.reference.hap1.scaffolds.renamed.fasta -I mountain_chickadee.filtered.sorted.nodup.bam -O moch_gatk.g.vcf.gz -ERC GVCF 4. Run GenotypeGVCFs. Code gatk --java-options &quot;-Djava.io.tmpdir=$SLURM_SCRATCH&quot; GenotypeGVCFs -R /scratch/alpine/mefr3284/ebio4100/reference/moch.dovetail.reference.hap1.scaffolds.renamed.fasta -V moch_gatk.g.vcf.gz -O moch_gatk.vcf.gz Notice that an index file was also made! (moch_gatk.g.vcf.gz.tbi) GATK has really good documentation, including this Best Practices workflow that describes the gold standard for genome processing. 4.3 Call variants with bcftools Another popular variant caller is bcftools mpileup. We will use the program bcftools a lot for processing VCF files. We are going to run both variant callers and compare the output. 5. Run bcftools mpileup and call. There are also two steps to this variant calling, but we can pipe from one to the other without making the large intermediate file. Code module load BCFtools bcftools mpileup -f /scratch/alpine/mefr3284/ebio4100/reference/moch.dovetail.reference.hap1.scaffolds.renamed.fasta mountain_chickadee.filtered.sorted.nodup.bam -a DP,AD -Ou | bcftools call -mv -Oz -f GQ -o moch_bcftools.vcf.gz tabix -fp vcf moch_bcftools.vcf.gz 4.4 Are they all the same? 6. Look at the files side by side. What is similar about them, and what is different? You can check their file sizes, use less to look at each one, and/or open each one in the OnDemand portal and look at them side-by-side that way. 7. Run bcftools stats to calculate differences. Code bcftools stats -s - moch_gatk.vcf.gz moch_bcftools.vcf.gz &gt; stats.txt Scroll through the output. There is a ton of information here, divided into sections. 8. Look at the ID section. One of the first sections is called “Definition of sets”. Here, ID numbers are described. The first two (0 and 1) should be describe things that are unique to each of the files, and the third one (2) should describe things that are the same between the files. Code grep -B2 &quot;^ID\\s&quot; stats.txt This command returns lines that start with (^) ID followed by white space. The -B2 flag returns the two lines prior to any lines that match the criteria. 9. Next, look at the section “Summary Numbers”. How many records do the VCF files have in common? Code grep -B1 &quot;^SN\\s&quot; stats.txt 10. Next, look at the section “Per-sample counts” or “PSC”. Code grep PSC stats.txt Below is a sample table (yours may or may not be identical). You will notice that bcftools called many more sites, and the category with the least agreement is the calling of indels (insertions or deletions). Calling indels is very tricky, and a big way for programs to diverge, especially when coverage is low. These files have artificially low coverage, and we will work with the higher coverage version of this dataset later. However, if this was our dataset, we would want to filter this dataset carefully. .scrolling { max-width: 1000px; overflow-x: auto; } |PSC|[2]id|[3]sample|[4]nRefHom|[5]nNonRefHom|[6]nHets|[7]nTransitions|[8]nTransversions|[9]nIndels|[10]average depth|[11]nSingletons|[12]nHapRef|[13]nHapAlt|[14]nMissing| |---|---|---|---|---|---|---|---|---|---|---|---|---|---| |PSC|0|moch21|0|94|0|43|51|227|1.9|321|0|0|0| |PSC|1|moch21|0|5263|0|3367|1896|202|1.0|5465|0|0|0| |PSC|2|moch21|0|2745|1|1878|868|30|2.0|2776|0|0|0| So which of these variant sets is right? The hard truth is that we don’t know the real genotypes– both of these files provide different hypotheses for the genotypes based on the alignment we provided. It is important to think carefully about your data, the analyses you are trying to do, and the assumptions of the model in order to decide which program is right for your analysis. 4.5 Assignment Create a script that will call variants in the BAM file you made in yesterday’s assignment (20H102_S21_L002_R2_001). You can use either program. When you are done, copy it to the folder $turnin/day_4/. Make sure you add your name to the file name! Try to run it on the cluster, it may take a few hours. Hint 1: Code Your header could look like: #!/bin/bash #SBATCH --nodes=1 #SBATCH --ntasks=1 #SBATCH --time=6:00:00 #SBATCH --partition=amilan #SBATCH --output=vcfs.out #SBATCH --qos=normal Hint 2: Code Make sure you have the names and locations of the files correct! If you would like more information on the programs we tried today, here are links to check out. GATK User Guide BCFtools Manual Picard Documentation "],["references.html", "References", " References Edgar, Robert C., and Serafim Batzoglou. 2006. “Multiple Sequence Alignment.” Current Opinion in Structural Biology 16 (3): 368–73. https://doi.org/https://doi.org/10.1016/j.sbi.2006.04.004. Katoh, Kazutaka, John Rozewicki, and Kazunori D Yamada. 2019. “MAFFT Online Service: Multiple Sequence Alignment, Interactive Sequence Choice and Visualization.” Briefings in Bioinformatics 20 (4): 1160–66. https://doi.org/10.1093/bib/bbx108. "]]
